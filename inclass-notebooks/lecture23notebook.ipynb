{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 23: Inference in Multiple Linear Regression \n",
    "***\n",
    "\n",
    "We'll need Numpy, Matplotlib, Pandas, and scipy.stats for this notebook, so let's load them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy import stats\n",
    "import statsmodels.api as sm \n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - The Problem of Multiple Comparisons \n",
    "*** \n",
    "\n",
    "In this exercise we'll explore the **Problem of Multiple Comparisons**.  In multiple linear regression, the PMC rears its ugly head when you try to perform inference on a large number of parameters based on a single data set.  In particular, it tells us that if we look for a statistically significant relationship between a large number of features of a dataset and the response, we're very likely to find evidence that at least one of the features is important just by random chance, even in the case that none of the features are important. This is what motivates us to use things like the full and partial $F$-tests instead. \n",
    "\n",
    "We'll explore this phenomenon using the data in pmc_data.csv. The data contains $n=200$ observations for a response $y$ and $p=20$ features $X_1, X_2, \\ldots, X_{20}$.  Load this data into a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfPMC = pd.read_csv(\"data/pmc_data.csv\")\n",
    "dfPMC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Let's start by doing some graphical exploration.  To look for relationships between an individual feature and the response we can make scatter plots.  Try several different features and see if any of them looks like they have a real relationship with the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the response and a single feature \n",
    "y, xk = dfPMC[\"y\"], dfPMC[\"X1\"]\n",
    "\n",
    "# Make Scatterplot \n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(10,8))\n",
    "ax.scatter(xk, y, color=\"steelblue\", s=100, alpha=0.75)\n",
    "ax.set_xlabel(\"x\",fontsize=16)\n",
    "ax.set_xlabel(\"y\",fontsize=16)\n",
    "ax.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Did you find any relevant relationships?  Probably not; because there aren't any.  In fact, the response $y$ was generated using the model \n",
    "\n",
    "$$\n",
    "Y = \\epsilon \\quad \\textrm{where} \\quad \\epsilon \\sim N(0,\\sigma^2)\n",
    "$$\n",
    "\n",
    "OK, but pretend we don't know this.  Let's construct a multiple linear regression on the data and examine the hypothesis tests for the individual parameters provided by statsmodel's summary feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract response vector y and feature matrix X from DataFrame\n",
    "y, X = dfPMC[\"y\"], dfPMC.iloc[:,1:]\n",
    "\n",
    "# Add constant to X \n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit OLS model \n",
    "model = sm.OLS(y, X).fit() \n",
    "\n",
    "# Print model summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Examine the p-values and 95% confidence intervals associated with each model parameter.  Do the tests indicate that any of the features have a statistically significant relationship with the response? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Now, look at the $F$-statistic and associated p-value for the MLR model.  Do these values indicate that at least one of the features has a statistically significant relationship with the response? How can you reconcile the result of **Part C** with this result? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
